from datetime import datetime

TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')
analysis_container = "gitlab-registry.cern.ch/cms-cmu/coffea4bees:fd85472f"   ### actual tag for reproducibility
combine_container = "gitlab-registry.cern.ch/cms-analysis/general/combine-container:CMSSW_11_3_4-combine_v9.1.0-harvester_v2.1.0"
dataset_location = "metadata/datasets_HH4b.yml"

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
#
rule all:
    input:
        # expand("output/histAll__{dataset}__{year}.coffea", year=config['year'], dataset=config['dataset']),
        expand("output/histsyst_{samplesyst}-{iysyst}.coffea", samplesyst=config['dataset_systematics'], iysyst=config['year']),
        #expand("output/histsystpreUL_{samplesystpreUL}-{iysystpreUL}.coffea", samplesystpreUL=config['dataset_preUL'], iysystpreUL=config['year_preUL']),
        "output/histMixedBkg_data_3b_for_mixed.coffea",
        "output/histMixedBkg_TT.coffea",
        "output/histMixedData.coffea",
        'output/histAll.coffea',
        'output/histAll_syst.coffea',
        "output/histAll_signals_cHHHX.coffea",
        'output/histSignal_UL.coffea',
        "output/RunII/passPreSel/fourTag/SB/nPVs.pdf",
        "output/datacards/shapes.root",
        "output/tmp.log",
        'output/datacards/limits.json',
        "output/datacards/plots/SvB_postfit.png"


rule analysis_databkgs:
    output: "output/histAll.coffea" 
    container: analysis_container
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset'],
        iy=config['year'],
        output="histAll.coffea",
        logname="histAll"
    resources:
        voms_proxy=True,
        kerberos=True,
        # compute_backend="htcondorcern",
        # htcondor_max_runtime="workday"
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
echo "Running {params.isam} {params.iy} - output {output}"
cd python/ 
mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o ../{output} -op ../output/ -m {dataset_location} --githash {params.hash} --gitdiff {params.diff}  #--dask
cd ../
mprof plot -o output/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp gitdiff.txt output/   ### only one time
#cp /tmp/coffea4bees-dask-report-* ../output/coffea4bees-dask-report_{params.logname}_{params.iy}.html
        """

#use rule analysis_databkgs as analysis_signals_preUL with:
#    output: "output/histAll_signal_preUL.coffea"
#    params:
#        hash=config['hash'],
#        diff=config['diff'],
#        isam=config['dataset_preUL'],
#        iy=config['year_preUL'],
#        output="histAll_signal_preUL.coffea",
#        logname="histAll_signal_preUL"

use rule analysis_databkgs as analysis_signals_mix with:
    output: "output/histSignal_UL.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=['ZZ4b', 'ZH4b'],
        iy=config['year'],
        output="histSignal_UL.coffea",
        logname="histSignal_UL"

use rule analysis_databkgs as analysis_mixedbkg_data3b with:
    output: "output/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="data_3b_for_mixed",
        iy=config['year_preUL'],
        output="histMixedBkg_data_3b_for_mixed.coffea",
        logname="mixedbkg_data3b"

use rule analysis_databkgs as analysis_mixedbkg with:
    output: "output/histMixedBkg_TT.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset_for_mixed'],
        iy=config['year'],
        output="histMixedBkg_TT.coffea",
        logname="mixedbkg_TT"

use rule analysis_databkgs as analysis_mixeddata with:
    output: "output/histMixedData.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="mixeddata",
        iy=config['year_preUL'],
        output="histMixedData.coffea",
        logname="mixeddata"


rule analysis_systematics:
    output:
        "output/histsyst_{samplesyst}-{iysyst}.coffea"
    container: analysis_container
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{samplesyst}",
        iy="{iysyst}",
        output="histsyst_{samplesyst}-{iysyst}.coffea",
        logname="syst_{samplesyst}-{iysyst}"
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
echo "Running {params.isam} {params.iy} - output {output}"
cd python/ 
mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o ../{output} -op ../output/ -m {dataset_location} --githash {params.hash} --gitdiff {params.diff}  -c analysis/metadata/HH4b_systematics.yml #--dask
cd ../
mprof plot -o output/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
        """

#use rule analysis_systematics as analysis_systematics_preUL with:
#    output:
#        "output/histsystpreUL_{samplesystpreUL}-{iysystpreUL}.coffea"
#    params:
#        hash=config['hash'],
#        diff=config['diff'],
#        isam="{samplesystpreUL}",
#        iy="{iysystpreUL}",
#        output="histsystpreUL_{samplesystpreUL}-{iysystpreUL}.coffea",
#        logname="syst_preUL_{samplesystpreUL}-{iysystpreUL}"

rule merging_coffea_files_databkgs:
   input:
    #    files = expand(['output/histAll__{dataset}__{iyear}.coffea'], iyear=config['year'], dataset=config['dataset'])
        files = expand(['output/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year']) #+ expand(['output/histsystpreUL_{idat}-{iyear}.coffea'], idat=config['dataset_preUL'], iyear=config['year_preUL'])
   output: 
        # "output/histAll.coffea"
       "output/histAll_syst.coffea"
   container: "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
   params:
    #    output= "histAll.coffea",
    #    logname= "databkgs"
        output= "histAll_syst.coffea",
        logname= "syst"
   resources:
       kerberos=True,
       compute_backend="kubernetes",
       kubernetes_memory_limit="9.5Gi"
   shell:
       """
mprof run -C -o python/mprofile_merge_{params.logname}.dat python python/analysis/merge_coffea_files.py -f {input.files} -o {output}
mprof plot -o output/mprofile_merge_{params.logname}.png python/mprofile_merge_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp {output} /eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
       """

# use rule merging_coffea_files_databkgs as rule merging_coffea_files_syst with:
#     input:
#         files = expand(['output/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year']) #+ expand(['output/histsystpreUL_{idat}-{iyear}.coffea'], idat=config['dataset_preUL'], iyear=config['year_preUL'])
#     output:
#         "output/histAll_syst.coffea"
#     params:
#         output= "histAll_syst.coffea",
#         logname= "syst"

use rule merging_coffea_files_databkgs as merging_coffea_files_signal_diffHHH with:
    input:
        files = expand(['output/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year'])

    output:
        "output/histAll_signals_cHHHX.coffea"
    params:
        output= "histAll_signals_cHHHX.coffea",
        logname= "signals_cHHHX"


rule make_plots:
    input:
        "output/histAll.coffea"
    output:
        "output/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    container: analysis_container
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
cd python/ 
mprof run -C -o mprofile_makeplots.dat python analysis/makePlots.py ../output/histAll.coffea -o ../output/ -m analysis/metadata/plotsAll.yml -s xW
mprof plot -o mprofile_makeplots.png ../output/mprofile_makeplots.dat
python .php-plots/bin/pb_deploy_plots.py ../output/RunII/ /eos/user/a/algomez/work/HH4b/reana/{TIMESTAMP}/ -r -c -j 4
        """
        
rule convert_hist_to_json:
    input:
        inall = "output/histAll.coffea",
        insyst = "output/histAll_syst.coffea",
        insystcHHHX = "output/histAll_signals_cHHHX.coffea"
    output:
        outall = "output/histAll.json",
        outsyst = "output/histAll_syst.json",
        outsystcHHHX = "output/histAll_signals_cHHHX.json"
    container: analysis_container
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
python python/stats_analysis/convert_hist_to_json.py -o {output.outall} -i {input.inall}
python python/stats_analysis/convert_hist_to_json.py -o {output.outsyst} -i {input.insyst} -s
python python/stats_analysis/convert_hist_to_json.py -o {output.outsystcHHHX} -i {input.insystcHHHX} -s
        """

rule make_combine_inputs:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals_cHHHX.json"
    output:
        "output/datacards/shapes.root",
        "output/datacards/datacard.txt"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
python3 python/stats_analysis/make_combine_inputs.py --classifier SvB_MA -f {input.injson} --syst_file {input.injsonsyst} --bkg_syst_file python/stats_analysis/hists/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin2.pkl --output_dir output/datacards/ --rebin 2 --metadata python/stats_analysis/metadata/HH4b.yml --make_syst_plots 
        """

rule run_combine:
    input:
        "output/datacards/datacard.txt"
    output:
        "output/datacards/limits.json"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
cat {input}
echo "RUNNING COMBINE"
source python/stats_analysis/run_combine.sh output/datacards/ 
source python/stats_analysis/run_combine.sh output/datacards/ --impacts
        """

rule run_postfit:
    input:
        "output/datacards/datacard.txt"
    output:
        "output/datacards/plots/SvB_postfit.png"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
cat {input}
echo "RUNNING COMBINE"
cd python/stats_analysis/
source run_combine.sh ../../output/datacards/ --postfit
python3 make_postfit_plot.py -i ../../output/datacards/postfit_s.root -o {output} --do_postfit False
        """

rule make_syst_plots:
    input:
        "output/datacards/shapes.root",
    log:
        "output/tmp.log"
    container: analysis_container
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
python python/.php-plots/bin/pb_deploy_plots.py output/datacards/ /eos/user/a/algomez/work/HH4b/reana/{TIMESTAMP}/ -r -c -j 4 -e pdf > {log}
        """
