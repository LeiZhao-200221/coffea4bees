from datetime import datetime

TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
#
rule all:
    input:
        expand("output/histdatabkgs_{sample}-{iy}.coffea", sample=config['dataset'], iy=config['year']),
        expand("output/histsyst_{samplesyst}-{iysyst}.coffea", samplesyst=config['dataset_systematics'], iysyst=config['year']),
        "output/histMixedBkg_data_3b_for_mixed.coffea",
        "output/histMixedBkg_TT.coffea",
        "output/histMixedData.coffea",
        'output/histAll.coffea',
        'output/histAll_syst.coffea',
        'output/histSignal.coffea',
        "output/RunII/passPreSel/fourTag/SB/nPVs.pdf"
        #"output/datacards/hists_SvB.root"


rule analysis_databkgs:
    output: "output/histdatabkgs_{sample}-{iy}.coffea"
    container: "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{sample}",
        iy="{iy}",
        output="histdatabkgs_{sample}-{iy}.coffea",
        logname="databkgs_{sample}_{iy}"
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
echo "Running {params.isam} {params.iy} - output {output}"
cd python/ 
mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o ../{output} -op ../output/ -m metadata/datasets_HH4b_cernbox.yml --githash {params.hash} --gitdiff {params.diff}  #--dask
cd ../
mprof plot -o output/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp gitdiff.txt output/   ### only one time
#cp /tmp/coffea4bees-dask-report-* ../output/coffea4bees-dask-report_{params.logname}_{params.iy}.html
        """

use rule analysis_databkgs as analysis_mixedbkg_data3b with:
    output: "output/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="data_3b_for_mixed",
        iy=config['year_mixed'],
        output="histMixedBkg_data_3b_for_mixed.coffea",
        logname="mixedbkg_data3b"

use rule analysis_databkgs as analysis_mixedbkg with:
    output: "output/histMixedBkg_TT.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset_for_mixed'],
        iy=config['year'],
        output="histMixedBkg_TT.coffea",
        logname="mixedbkg_TT"

use rule analysis_databkgs as analysis_mixeddata with:
    output: "output/histMixedData.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="mixeddata",
        iy=config['year_mixed'],
        output="histMixedData.coffea",
        logname="mixeddata"


rule analysis_systematics:
    output:
        "output/histsyst_{samplesyst}-{iysyst}.coffea"
    container:
        "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{samplesyst}",
        iy="{iysyst}",
        output="histsyst_{samplesyst}-{iysyst}.coffea",
        logname="syst_{samplesyst}-{iysyst}"
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
echo "Running {params.isam} {params.iy} - output {output}"
cd python/ 
mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o ../{output} -op ../output/ -m metadata/datasets_HH4b_cernbox.yml --githash {params.hash} --gitdiff {params.diff}  -c analysis/metadata/HH4b_systematics.yml #--dask
cd ../
mprof plot -o output/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
        """


rule merging_coffea_files_databkgs:
    input:
        files = expand(['output/histdatabkgs_{idat}-{iyear}.coffea'], idat=config['dataset'], iyear=config['year'])
    output: "output/histAll.coffea"
    container: "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
    params:
        output= "histAll.coffea",
        logname= "databkgs"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
mprof run -C -o python/mprofile_merge_{params.logname}.dat python python/analysis/merge_coffea_files.py -f {input.files} -o {output}
mprof plot -o output/mprofile_merge_{params.logname}.png python/mprofile_merge_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp {output} /eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
        """

use rule merging_coffea_files_databkgs as merging_coffea_files_syst with:
    input:
        files = expand(['output/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year'])
    output:
        "output/histAll_syst.coffea"
    params:
        output= "histAll_syst.coffea",
        logname= "syst"

use rule merging_coffea_files_databkgs as merging_coffea_files_signal with:
    input:
        files = expand(['output/histdatabkgs_{idat}-{iyear}.coffea'], idat=['HH4b', 'ZH4b', 'ZZ4b'], iyear=config['year'])
    output:
        "output/histSignal.coffea"
    params:
        output= "histSignal.coffea",
        logname= "signal"


rule make_plots:
    input:
        "output/histAll.coffea"
    output:
        "output/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    container:
        "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
cd python/ 
mprof run -C -o mprofile_makeplots.dat python analysis/makePlots.py ../output/histAll.coffea -o ../output/ -m analysis/metadata/plotsAll.yml
mprof plot -o mprofile_makeplots.png ../output/mprofile_makeplots.dat
python .php-plots/bin/pb_deploy_plots.py ../output/RunII/ /eos/user/a/algomez/work/HH4b/reana/{TIMESTAMP}/ -r -c
        """
        
#rule convert_hist_to_yml:
#    input:
#        inall = "output/histAll.coffea",
#        insyst = "output/histAll_syst.coffea"
#    output:
#        outall = "output/histAll.yml",
#        outsyst = "output/histAll_systematics.yml"
#    container:
#        "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
#    resources:
#        compute_backend="kubernetes",
#        kubernetes_memory_limit="8Gi"
#    shell:
#        """
#python python/stats_analysis/convert_hist_to_yaml.py -o {output.outall} -i {input.inall}
#python python/stats_analysis/convert_hist_to_yaml.py -o {output.outsyst} -i {input.insyst} -s
#        """
#
#rule convert_yml_to_hist:
#    input:
#        "output/histAll.yml"
#    output:
#        "output/datacards/hists_SvB.root"
#    container:
#        "gitlab-registry.cern.ch/cms-cloud/combine-standalone:v9.2.0"
#    resources:
#        compute_backend="kubernetes"
#    shell:
#        """
#python python/stats_analysis/convert_yml_to_root.py --classifier SvB_MA SvB -f output/histAll.yml --merge2016 --output_dir output/datacards/ --plot --make_combine_inputs
#        """
