from datetime import datetime
import os

# Define username once for reuse throughout the workflow
USERNAME = os.getenv("USER", "coffea4bees_default")

# Import rule modules
module analysis:
    snakefile: "rules/analysis.smk"
    config: config

module stat_analysis:
    snakefile: "rules/stat_analysis.smk"
    config: config

module combine:
    snakefile: "rules/combine.smk"
    config: config

include: "helpers/common.smk"

hash = config['extra_arguments'].split("--githash ")[1]
EOS_OUTPUT = f"/eos/user/a/algomez/work/HH4b/reana/{datetime.now().strftime('%Y%m%d')}_{hash}/"

# Filter dataset_systematics by signal type
hh4b_datasets = [d for d in config['dataset_systematics'] if d.startswith('GluGlu')]
zz4b_datasets = [d for d in config['dataset_systematics'] if 'ZZ4b' in d]
zh4b_datasets = [d for d in config['dataset_systematics'] if 'ZH4b' in d]

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
# #
rule final_rule:
    input:
        "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf",
        expand(
            "output/datacards/{channel}/limits__{signallabel}.json",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        expand(
            "output/datacards/{channel}/significance_observed__{signallabel}.txt",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        expand(
            "output/datacards/{channel}/impacts__{signallabel}.pdf",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        expand(
            "output/datacards/{channel}/postfit__{signallabel}.pdf",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        expand(
            "output/datacards/{channel}/likelihood_scan__{signallabel}.pdf",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        expand(
            "output/datacards/{channel}/gof__{signallabel}.pdf",
            zip,
            channel=list(config['channels'].keys()),
            signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()]
        ),
        "output/datacards/HH4b/systs/SvB_MA_ps_hh_nominal.pdf",
        "output/datacards/ZZ4b/systs/SvB_MA_ps_zz_nominal.pdf",
        "output/datacards/ZH4b/systs/SvB_MA_ps_zh_nominal.pdf",

        # expand("output/singlefiles/histNoJCM__{sample}-{year}.coffea", sample=config['dataset'], year=config['year'])  ### trick to define wildcards
    container: config["analysis_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
    shell: 
        """
        cp gitdiff.txt output/
        echo "Copying output to cernbox"
        python base_class/php_plots/pb_deploy_plots.py output/ {EOS_OUTPUT} -r -c -j 4
        mkdir -p {EOS_OUTPUT}
        cp -r output/* {EOS_OUTPUT}
        """

# use rule analysis_processor from analysis as analysis with:
#     output: "output/singlefiles/histNoJCM__{sample}-{year}.coffea"
#     params:
#         datasets="{sample}",
#         years="{year}",
#         metadata="python/analysis/metadata/HH4b_noJCM.yml",
#         processor="python/analysis/processors/processor_HH4b.py",
#         datasets_file=config['dataset_location'],
#         blind=False,
#         run_performance=True,
#         hash=config['hash'],
#         diff=config['diff'],
#         username=USERNAME
#     resources:
#         voms_proxy=True,
#         kerberos=True,
#         compute_backend="kubernetes",
#         kubernetes_memory_limit="9.5Gi"
#     log: "output/logs/analysis_histNoJCM__{sample}-{year}.log"

# use rule merging_coffea_files from analysis as merging_coffea_files with:
#     input: expand(['output/singlefiles/histNoJCM__{sample}-{year}.coffea'], sample=config['dataset'], year=config['year']) 
#     output: "output/histNoJCM.coffea"
#     resources:
#         kerberos=True,
#         compute_backend="kubernetes",
#         kubernetes_memory_limit="9.5Gi"
#     log: "output/logs/merging_histNoJCM.log"

# use rule make_JCM from analysis as make_JCM with:
#     input: "output/histNoJCM.coffea"
#     output: "output/JCM/jetCombinatoricModel_SB_reana.yml"
#     resources:
#         voms_proxy=True,
#         kerberos=True,
#         compute_backend="kubernetes",
#     log: "output/logs/make_JCM.log"

#######
### Running analysis processor
#######

### In the next rules, the input is commented out to not run JCM again. 
use rule analysis_processor from analysis as analysis_databkgs with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/hist__{sample}-{year}.coffea"
    params:
        datasets="{sample}",
        years="{year}",
        metadata="python/analysis/metadata/HH4b.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: "output/logs/analysis_hist__{sample}-{year}.log"


use rule analysis_databkgs as analysis_data with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histdata__data-{year}.coffea"
    params:
        datasets="data",
        years="{year}",
        metadata="python/analysis/metadata/HH4b.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=lambda wildcards: f"{config['extra_arguments']} -e {config['data_eras'][wildcards.year]}",

        username=USERNAME
    log: "output/logs/analysis_histdata__data-{year}.log"


use rule analysis_databkgs as analysis_data_UL17B with:
    output: "output/singlefiles/histdata__data-UL17B.coffea"
    params:
        datasets="data",
        years="UL17",
        metadata="python/analysis/metadata/HH4b_dataUL17B.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} -e B",

        username=USERNAME
    log: "output/logs/analysis_histdata__data-UL17B.log"

use rule analysis_databkgs as analysis_signals with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsignal__{sample_signal}-{year}.coffea"
    params:
        datasets="{sample_signal}",
        years="{year}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: "output/logs/analysis_histsignal__{sample_signal}-{year}.log"

use rule analysis_databkgs as analysis_mixedbkg_data3b with:
    output: "output/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        datasets="data_3b_for_mixed",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_mixed_data.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: "output/logs/analysis_mixedbkg_data3b.log"


use rule analysis_databkgs as analysis_mixedbkg with:
    output: "output/histMixedBkg_TT.coffea"
    params:
        datasets=config['dataset_for_mixed'],
        years=config['year'],
        metadata="python/analysis/metadata/HH4b_nottcheck.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: "output/logs/analysis_mixedbkg_TT.log"

use rule analysis_databkgs as analysis_mixeddata with:
    output: "output/histMixedData.coffea"
    params:
        datasets="mixeddata",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_nottcheck.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: "output/logs/analysis_mixeddata.log"


use rule analysis_databkgs as analysis_systematics_others with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsyst_others_{samplesyst}-{iysyst}.coffea"
    params:
        datasets="{samplesyst}",
        years="{iysyst}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} --systematics others",
        username=USERNAME
    log: "output/logs/analysis_histsyst_others_{samplesyst}-{iysyst}.log"


use rule analysis_databkgs as analysis_systematics_jes with:
    # input: "output/JCM/jetCombinatoricModel_SB_reana.yml"
    output: "output/singlefiles/histsyst_jes_{samplesyst}-{iysyst}.coffea"
    params:
        datasets="{samplesyst}",
        years="{iysyst}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} --systematics jes",
        username=USERNAME
    log: "output/logs/analysis_histsyst_jes_{samplesyst}-{iysyst}.log"

#######
### Merging histograms 
#######

use rule merging_coffea_files from analysis as merging_coffea_files_syst_HH4b with:
    input: expand(['output/singlefiles/histsyst_others_{idatsyst}-{iyear}.coffea'], idatsyst=hh4b_datasets, iyear=config['year']) + expand(['output/singlefiles/histsyst_jes_{idatsyst}-{iyear}.coffea'], idatsyst=hh4b_datasets, iyear=config['year'])
    output: "output/histAll_signals__HH4b.coffea"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    params:
        run_performance=True
    log: "output/logs/merging_signals_HH4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_syst_ZZ4b with:
    input: expand(['output/singlefiles/histsyst_others_{idatsyst}-{iyear}.coffea'], idatsyst=zz4b_datasets, iyear=config['year']) + expand(['output/singlefiles/histsyst_jes_{idatsyst}-{iyear}.coffea'], idatsyst=zz4b_datasets, iyear=config['year'])
    output: "output/histAll_signals__ZZ4b.coffea"
    log: "output/logs/merging_signals_ZZ4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_syst_ZH4b with:
    input: expand(['output/singlefiles/histsyst_others_{idatsyst}-{iyear}.coffea'], idatsyst=zh4b_datasets, iyear=config['year']) + expand(['output/singlefiles/histsyst_jes_{idatsyst}-{iyear}.coffea'], idatsyst=zh4b_datasets, iyear=config['year'])
    output: "output/histAll_signals__ZH4b.coffea"
    log: "output/logs/merging_signals_ZH4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_histAll with:
    input: expand(['output/singlefiles/histsignal__{sample_signal}-{year}.coffea'], sample_signal=config['dataset_signals'], year=config['year']) + expand(['output/singlefiles/hist__{idat}-{iyear}.coffea'], idat=config['dataset_tt'], iyear=config['year']) + expand(['output/singlefiles/histdata__data-{iyear}.coffea'], iyear=config['year']) + [ 'output/singlefiles/histdata__data-UL17B.coffea' ]
    output: "output/histAll.coffea"
    log: "output/logs/merging_histAll.log"

########
### Making plots
########    

use rule make_plots from analysis as make_plots with:
    input: "output/histAll.coffea"
    output: "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: "output/logs/make_plots.log"

########
### Converting histograms to JSON and ROOT formats
########

use rule convert_hist_to_json from stat_analysis as convert_hist_to_json_histAll with:
    input: "output/histAll.coffea"
    output: "output/histAll.json"
    log: "output/logs/convert_hist_to_json_histAll.log"

use rule convert_hist_to_json_histAll as convert_hist_to_json_signals with:
    input: "output/histAll_signals__{channel}.coffea"
    output: "output/histAll_signals__{channel}.json"
    params:
        syst_flag = "-s"
    log: "output/logs/convert_hist_to_json_signals__{channel}.log"

use rule convert_hist_to_json_closure from stat_analysis as convert_hist_to_json_mixdata3b with:
    input: "output/histMixedBkg_data_3b_for_mixed.coffea"
    output: "output/histMixedBkg_data_3b_for_mixed.json"
    log: "output/logs/convert_hist_to_json_mixdata3b.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixbkgtt with:
    input: "output/histMixedBkg_TT.coffea"
    output: "output/histMixedBkg_TT.json"
    log: "output/logs/convert_hist_to_json_mixbkgtt.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixdata with:
    input: "output/histMixedData.coffea"
    output: "output/histMixedData.json"
    log: "output/logs/convert_hist_to_json_mixdata.log"

use rule convert_json_to_root from stat_analysis as convert_json_to_root_TT with:
    input: "output/histMixedBkg_TT.json"
    output: "output/histMixedBkg_TT.root"
    container: config["combine_container"]
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: "output/logs/convert_json_to_root_TT.log"

use rule convert_json_to_root_TT as convert_json_to_root_mix with:
    input: "output/histMixedData.json"
    output: "output/histMixedData.root"
    log: "output/logs/convert_json_to_root_mix.log"

use rule convert_json_to_root_TT as convert_json_to_root_histall with:
    input: "output/histAll.json"
    output: "output/histAll.root"
    log: "output/logs/convert_json_to_root_histall.log"

use rule convert_json_to_root_TT as convert_json_to_root_data3b with:
    input: "output/histMixedBkg_data_3b_for_mixed.json"
    output: "output/histMixedBkg_data_3b_for_mixed.root"
    log: "output/logs/convert_json_to_root_data3b.log"

#######
### Closure fits (background systematics)
########

use rule run_two_stage_closure from stat_analysis as run_two_stage_closure_HH4b with:
    input: 
        file_TT = "output/histMixedBkg_TT.root",
        file_mix = "output/histMixedData.root",
        file_sig = "output/histAll.root",
        file_data3b = "output/histMixedBkg_data_3b_for_mixed.root"
    output: "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    container: config["combine_container"]
    params:
        outputPath = "output/closureFits/",
        rebin = "1",
        variable = "SvB_MA_ps_hh",
        variable_binning = "",
        container_wrapper = config['container_wrapper']
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: "output/logs/run_two_stage_closure_HH4b.log"

use rule run_two_stage_closure_HH4b as run_two_stage_closure_ZZ4b with:
    output: "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zz/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zz_rebin1.pkl"
    params:
        outputPath = "output/closureFits",
        rebin = "1",
        variable = "SvB_MA_ps_zz",
        variable_binning = "",
        container_wrapper = config['container_wrapper']
    log: "output/logs/run_two_stage_closure_ZZ4b.log"

use rule run_two_stage_closure_HH4b as run_two_stage_closure_ZH4b with:
    output: "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zh_rebin1.pkl"
    params:
        outputPath = "output/closureFits",
        rebin = "1",
        variable = "SvB_MA_ps_zh",
        variable_binning = "",
        container_wrapper = config['container_wrapper']
    log: "output/logs/run_two_stage_closure_ZH4b.log"

########
### Making combine inputs and datacards
########

use rule make_combine_inputs from stat_analysis as make_combine_inputs_HH4b with:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals__HH4b.json", 
        bkgsyst = "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    output: "output/datacards/HH4b/datacard__HH4b.txt"
    container: config["combine_container"]
    params:
        variable= "SvB_MA.ps_hh",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/HH4b.yml",
        output_dir="output/datacards/HH4b/",
        variable_binning="",
        stat_only="",
        signal="HH4b",
        container_wrapper = config['container_wrapper']
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: "output/logs/make_combine_inputs_HH4b.log"

use rule make_combine_inputs_HH4b as make_combine_inputs_ZZ4b with:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals__ZZ4b.json", 
        bkgsyst = "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zz/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zz_rebin1.pkl"
    output: "output/datacards/ZZ4b/datacard__ZZ4b.txt"
    params:
        variable= "SvB_MA.ps_zz",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/ZZ4b.yml",
        output_dir="output/datacards/ZZ4b/",
        variable_binning="",
        stat_only="",
        signal="HH4b",
        container_wrapper = config['container_wrapper']
    log: "output/logs/make_combine_inputs_ZZ4b.log"

use rule make_combine_inputs_HH4b as make_combine_inputs_ZH4b with:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals__ZH4b.json", 
        bkgsyst = "output/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zh_rebin1.pkl"
    output: "output/datacards/ZH4b/datacard__ZH4b.txt"
    params:
        variable= "SvB_MA.ps_zh",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/ZH4b.yml",
        output_dir="output/datacards/ZH4b/",
        variable_binning="",
        stat_only="",
        signal="ZH4b",
        container_wrapper = config['container_wrapper']
    log: "output/logs/make_combine_inputs_ZH4b.log"

use rule workspace from combine as workspace with:
    input: "output/datacards/{channel}/datacard__{channel}.txt"
    output: "output/datacards/{channel}/datacard__{signallabel}.root"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        othersignal_maps=lambda wildcards: additional_poi(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/workspace_{channel}__{signallabel}.log"

use rule limits from combine as limits with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output: 
        txt="output/datacards/{channel}/limits__{signallabel}.txt",
        json="output/datacards/{channel}/limits__{signallabel}.json"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/limits_{channel}__{signallabel}.log"

use rule significance from combine as significance with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output:
        observed="output/datacards/{channel}/significance_observed__{signallabel}.txt",
        expected="output/datacards/{channel}/significance_expected__{signallabel}.txt"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/significance_{channel}__{signallabel}.log"

use rule impacts from combine as impacts with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output: "output/datacards/{channel}/impacts__{signallabel}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        set_parameters_ranges=lambda wildcards: set_parameters_ranges(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: "output/logs/impacts_{channel}_datacard_{channel}__{signallabel}.log"

use rule postfit from combine as postfit_generic with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output: "output/datacards/{channel}/postfit__{signallabel}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/postfit_{channel}_datacard_{channel}__{signallabel}.log"

use rule likelihood_scan from combine as likelihood_scan with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output: "output/datacards/{channel}/likelihood_scan__{signallabel}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/likelihood_scan_{channel}_datacard_{channel}__{signallabel}.log"

use rule gof from combine as gof with:
    input: "output/datacards/{channel}/datacard__{signallabel}.root"
    output: "output/datacards/{channel}/gof__{signallabel}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: "output/logs/gof_{channel}_datacard_{channel}__{signallabel}.log"

use rule make_syst_plots from stat_analysis as make_syst_plots_HH4b with:
    input: "output/datacards/HH4b/datacard__HH4b.txt"
    output: "output/datacards/HH4b/systs/SvB_MA_ps_hh_nominal.pdf"
    container: config["combine_container"]
    params:
        variable="SvB_MA_ps_hh",
        output_dir="output/datacards/HH4b/",
        container_wrapper = config['container_wrapper']
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: "output/logs/make_syst_plots_HH4b.log"

use rule make_syst_plots_HH4b as make_syst_plots_ZZ4b with:
    input: "output/datacards/ZZ4b/datacard__ZZ4b.txt"
    output: "output/datacards/ZZ4b/systs/SvB_MA_ps_zz_nominal.pdf"
    params:
        variable="SvB_MA_ps_zz",
        output_dir="output/datacards/ZZ4b/",
        container_wrapper = config['container_wrapper']
    log: "output/logs/make_syst_plots_ZZ4b.log"

use rule make_syst_plots_HH4b as make_syst_plots_ZH4b with:
    input: "output/datacards/ZH4b/datacard__ZH4b.txt"
    output: "output/datacards/ZH4b/systs/SvB_MA_ps_zh_nominal.pdf"
    params:
        variable="SvB_MA_ps_zh",
        output_dir="output/datacards/ZH4b/",
        container_wrapper = config['container_wrapper']
    log: "output/logs/make_syst_plots_ZH4b.log"