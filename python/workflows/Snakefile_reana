from datetime import datetime
import os

# Define username once for reuse throughout the workflow
USERNAME = os.getenv("USER", "coffea4bees_default")

# Import rule modules
module analysis:
    snakefile: "rules/analysis.smk"
    config: config

module stat_analysis:
    snakefile: "rules/stat_analysis.smk"
    config: config

module combine:
    snakefile: "rules/combine.smk"
    config: config

include: "helpers/common.smk"

hash = config['extra_arguments'].split("--githash ")[1]
EOS_OUTPUT = f"/eos/user/a/algomez/work/HH4b/reana/{datetime.now().strftime('%Y%m%d')}_{hash}/"

# Filter dataset_systematics by signal type
hh4b_datasets = [d for d in config['dataset_systematics'] if d.startswith('GluGlu')]
zz4b_datasets = [d for d in config['dataset_systematics'] if 'ZZ4b' in d]
zh4b_datasets = [d for d in config['dataset_systematics'] if 'ZH4b' in d]

output_combine = [
    f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.json",
    f"{config['output_path']}/datacards/{{channel}}/significance__{{signallabel}}.log",
    f"{config['output_path']}/datacards/{{channel}}/impacts__{{signallabel}}.pdf",
    f"{config['output_path']}/datacards/{{channel}}/likelihood_scan__{{signallabel}}.pdf",
    f"{config['output_path']}/datacards/{{channel}}/gof__{{signallabel}}.pdf",
]

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
# #
rule final_rule:
    input:
        f"{config['output_path']}/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf",
        # expand(
        #     f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/{{channellabel}}/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_{{channellabel}}_rebin1.pkl", zip,
        #     channel=list(config['channels'].keys()),
        #     channellabel=[config['channels'][k]['channellabel'] for k in config['channels'].keys()]
        # ),
        [expand(pattern, zip,
                channel=list(config['channels'].keys()),
                signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()],
                channellabel=[k.lower().split('4b')[0] for k in config['channels'].keys()])
        for pattern in output_combine],
        f"{config['output_path']}/datacards/ZH4b/plots/postfitplots__ZH_bbbb__prefit.pdf",
        f"{config['output_path']}/datacards/ZZ4b/plots/postfitplots__ZZ_bbbb__prefit.pdf",
        f"{config['output_path']}/datacards/HH4b/plots/postfitplots__ggHH_kl_1_kt_1_13p0TeV_hbbhbb__prefit.pdf",
        # f"{config['output_path']}/datacards/ZH4b/systs/SvB_MA_ps_zh_nominal.pdf",
        # f"{config['output_path']}/datacards/ZZ4b/systs/SvB_MA_ps_zz_nominal.pdf",
        # f"{config['output_path']}/datacards/HH4b/systs/SvB_MA_ps_hh_nominal.pdf"
    container: config["analysis_container"]
    resources:
        voms_proxy=True,
        kerberos=True,
    shell: 
        """
        cp gitdiff.txt output/
        echo "Copying output to cernbox"
        python src/plots/pb_deploy_plots.py output/ {EOS_OUTPUT} -r -c -j 4
        mkdir -p {EOS_OUTPUT}
        cp -r output/* {EOS_OUTPUT}
        """

# use rule analysis_processor from analysis as analysis with:
#     output: f"{config['output_path']}/singlefiles/histNoJCM__{sample}-{year}.coffea"
#     params:
#         datasets="{sample}",
#         years="{year}",
#         metadata="python/analysis/metadata/HH4b_noJCM.yml",
#         processor="python/analysis/processors/processor_HH4b.py",
#         datasets_file=config['dataset_location'],
#         blind=False,
#         run_performance=True,
#         hash=config['hash'],
#         diff=config['diff'],
#         username=USERNAME
#     resources:
#         voms_proxy=True,
#         kerberos=True,
#         compute_backend="kubernetes",
#         kubernetes_memory_limit="9.5Gi"
#     log: f"{config['output_path']}/logs/analysis_histNoJCM__{sample}-{year}.log"

# use rule merging_coffea_files from analysis as merging_coffea_files with:
#     input: expand([f'{config["output_path"]}/singlefiles/histNoJCM__{sample}-{year}.coffea'], sample=config['dataset'], year=config['year']) 
#     output: f"{config['output_path']}/histNoJCM.coffea"
#     resources:
#         kerberos=True,
#         compute_backend="kubernetes",
#         kubernetes_memory_limit="9.5Gi"
#     log: f"{config['output_path']}/logs/merging_histNoJCM.log"

# use rule make_JCM from analysis as make_JCM with:
#     input: f"{config['output_path']}/histNoJCM.coffea"
#     output: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
#     resources:
#         voms_proxy=True,
#         kerberos=True,
#         compute_backend="kubernetes",
#     log: f"{config['output_path']}/logs/make_JCM.log"

#######
### Running analysis processor
#######

### In the next rules, the input is commented out to not run JCM again. 
use rule analysis_processor from analysis as analysis_databkgs with:
    # input: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
    output: f"{config['output_path']}/singlefiles/hist__{{sample}}-{{year}}.coffea"
    params:
        datasets="{sample}",
        years="{year}",
        metadata="python/analysis/metadata/HH4b.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: f"{config['output_path']}/logs/analysis_hist__{{sample}}-{{year}}.log"


use rule analysis_databkgs as analysis_data with:
    # input: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
    output: f"{config['output_path']}/singlefiles/histdata__data-{{year}}.coffea"
    params:
        datasets="data",
        years="{year}",
        metadata="python/analysis/metadata/HH4b.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=lambda wildcards: f"{config['extra_arguments']} -e {config['data_eras'][wildcards.year]}",
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_histdata__data-{{year}}.log"


use rule analysis_databkgs as analysis_data_UL17B with:
    output: f"{config['output_path']}/singlefiles/histdata__data-UL17B.coffea"
    params:
        datasets="data",
        years="UL17",
        metadata="python/analysis/metadata/HH4b_dataUL17B.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} -e B",

        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_histdata__data-UL17B.log"

use rule analysis_databkgs as analysis_signals with:
    # input: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
    output: f"{config['output_path']}/singlefiles/histsignal__{{sample_signal}}-{{year}}.coffea"
    params:
        datasets="{sample_signal}",
        years="{year}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_histsignal__{{sample_signal}}-{{year}}.log"

### mixdata for HH
use rule analysis_databkgs as analysis_mixedbkg_data3b with:
    output: f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        datasets="data_3b_for_mixed",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_mixed_data.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixedbkg_data3b.log"


use rule analysis_databkgs as analysis_mixedbkg with:
    output: f"{config['output_path']}/histMixedBkg_TT.coffea"
    params:
        datasets=config['dataset_for_mixed'],
        years=config['year'],
        metadata="python/analysis/metadata/HH4b_nottcheck.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixedbkg_TT.log"

use rule analysis_databkgs as analysis_mixeddata with:
    output: f"{config['output_path']}/histMixedData.coffea"
    params:
        datasets="mixeddata",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_nottcheck.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixeddata.log"


### mixeddata for ZZ/ZH
use rule analysis_databkgs as analysis_mixedbkg_data3b_ZZZH with:
    output: f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.coffea"
    params:
        datasets="data_3b_for_mixed",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_mixed_data_ZZZH.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixedbkg_data3b_ZZZH.log"


use rule analysis_databkgs as analysis_mixedbkg_ZZZH with:
    output: f"{config['output_path']}/histMixedBkg_ZZZH_TT.coffea"
    params:
        datasets=config['dataset_for_mixed'],
        years=config['year'],
        metadata="python/analysis/metadata/HH4b_mixed_data_ZZZH.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixedbkg_TT_ZZZH.log"

use rule analysis_databkgs as analysis_mixeddata_ZZZH with:
    output: f"{config['output_path']}/histMixedData_ZZZH.coffea"
    params:
        datasets="mixeddata",
        years=config['year_preUL'],
        metadata="python/analysis/metadata/HH4b_mixed_data_ZZZH.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=config['extra_arguments'],
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_mixeddata.log"

use rule analysis_databkgs as analysis_systematics_others with:
    # input: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
    output: f"{config['output_path']}/singlefiles/histsyst_others_{{samplesyst}}-{{iysyst}}.coffea"
    params:
        datasets="{samplesyst}",
        years="{iysyst}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} --systematics others",
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_histsyst_others_{{samplesyst}}-{{iysyst}}.log"


use rule analysis_databkgs as analysis_systematics_jes with:
    # input: f"{config['output_path']}/JCM/jetCombinatoricModel_SB_reana.yml"
    output: f"{config['output_path']}/singlefiles/histsyst_jes_{{samplesyst}}-{{iysyst}}.coffea"
    params:
        datasets="{samplesyst}",
        years="{iysyst}",
        metadata="python/analysis/metadata/HH4b_signals.yml",
        processor="python/analysis/processors/processor_HH4b.py",
        datasets_file=config['dataset_location'],
        blind=False,
        run_performance=True,
        extra_arguments=f"{config['extra_arguments']} --systematics jes",
        username=USERNAME
    log: f"{config['output_path']}/logs/analysis_histsyst_jes_{{samplesyst}}-{{iysyst}}.log"

#######
### Merging histograms 
#######

use rule merging_coffea_files from analysis as merging_coffea_files_syst_HH4b with:
    input: expand([f'{config["output_path"]}/singlefiles/histsyst_others_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=hh4b_datasets, iyear=config['year']) + expand([f'{config["output_path"]}/singlefiles/histsyst_jes_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=hh4b_datasets, iyear=config['year'])
    output: f"{config['output_path']}/histAll_signals__HH4b.coffea"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    params:
        run_performance=True
    log: f"{config['output_path']}/logs/merging_signals_HH4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_syst_ZZ4b with:
    input: expand([f'{config["output_path"]}/singlefiles/histsyst_others_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=zz4b_datasets, iyear=config['year']) + expand([f'{config["output_path"]}/singlefiles/histsyst_jes_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=zz4b_datasets, iyear=config['year'])
    output: f"{config['output_path']}/histAll_signals__ZZ4b.coffea"
    log: f"{config['output_path']}/logs/merging_signals_ZZ4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_syst_ZH4b with:
    input: expand([f'{config["output_path"]}/singlefiles/histsyst_others_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=zh4b_datasets, iyear=config['year']) + expand([f'{config["output_path"]}/singlefiles/histsyst_jes_{{idatsyst}}-{{iyear}}.coffea'], idatsyst=zh4b_datasets, iyear=config['year'])
    output: f"{config['output_path']}/histAll_signals__ZH4b.coffea"
    log: f"{config['output_path']}/logs/merging_signals_ZH4b.log"

use rule merging_coffea_files_syst_HH4b as merging_coffea_files_histAll with:
    input: expand([f'{config["output_path"]}/singlefiles/histsignal__{{sample_signal}}-{{year}}.coffea'], sample_signal=config['dataset_signals'], year=config['year']) + expand([f'{config["output_path"]}/singlefiles/hist__{{idat}}-{{iyear}}.coffea'], idat=config['dataset_tt'], iyear=config['year']) + expand([f'{config["output_path"]}/singlefiles/histdata__data-{{iyear}}.coffea'], iyear=config['year']) + [ f'{config["output_path"]}/singlefiles/histdata__data-UL17B.coffea' ]
    output: f"{config['output_path']}/histAll.coffea"
    log: f"{config['output_path']}/logs/merging_histAll.log"

########
### Making plots
########    

use rule make_plots from analysis as make_plots with:
    input: f"{config['output_path']}/histAll.coffea"
    output: f"{config['output_path']}/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: f"{config['output_path']}/logs/make_plots.log"

########
### Converting histograms to JSON and ROOT formats
########

use rule convert_hist_to_json from stat_analysis as convert_hist_to_json_histAll with:
    input: f"{config['output_path']}/histAll.coffea"
    output: f"{config['output_path']}/histAll.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_histAll.log"

use rule convert_hist_to_json_histAll as convert_hist_to_json_signals with:
    input: f"{config['output_path']}/histAll_signals__{{channel}}.coffea"
    output: f"{config['output_path']}/histAll_signals__{{channel}}.json"
    params:
        syst_flag="-s"
    log: f"{config['output_path']}/logs/convert_hist_to_json_signals__{{channel}}.log"

use rule convert_hist_to_json_closure from stat_analysis as convert_hist_to_json_mixdata3b with:
    input: f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.coffea"
    output: f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixdata3b.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixbkgtt with:
    input: f"{config['output_path']}/histMixedBkg_TT.coffea"
    output: f"{config['output_path']}/histMixedBkg_TT.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixbkgtt.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixdata with:
    input: f"{config['output_path']}/histMixedData.coffea"
    output: f"{config['output_path']}/histMixedData.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixdata.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixdata3b_ZZZH with:
    input: f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.coffea"
    output: f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixdata3b_ZZZH.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixbkgtt_ZZZH with:
    input: f"{config['output_path']}/histMixedBkg_ZZZH_TT.coffea"
    output: f"{config['output_path']}/histMixedBkg_ZZZH_TT.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixbkgtt_ZZZH.log"

use rule convert_hist_to_json_mixdata3b as convert_hist_to_json_mixdata_ZZZH with:
    input: f"{config['output_path']}/histMixedData_ZZZH.coffea"
    output: f"{config['output_path']}/histMixedData_ZZZH.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_mixdata_ZZZH.log"

use rule convert_json_to_root from stat_analysis as convert_json_to_root_TT with:
    input: f"{config['output_path']}/histMixedBkg_TT.json"
    output: f"{config['output_path']}/histMixedBkg_TT.root"
    container: config["combine_container"]
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: f"{config['output_path']}/logs/convert_json_to_root_TT.log"

use rule convert_json_to_root_TT as convert_json_to_root_mix with:
    input: f"{config['output_path']}/histMixedData.json"
    output: f"{config['output_path']}/histMixedData.root"
    log: f"{config['output_path']}/logs/convert_json_to_root_mix.log"

use rule convert_json_to_root_TT as convert_json_to_root_data3b with:
    input: f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.json"
    output: f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.root"
    log: f"{config['output_path']}/logs/convert_json_to_root_data3b.log"

use rule convert_json_to_root_TT as convert_json_to_root_TT_ZZZH with:
    input: f"{config['output_path']}/histMixedBkg_ZZZH_TT.json"
    output: f"{config['output_path']}/histMixedBkg_ZZZH_TT.root"
    container: config["combine_container"]
    log: f"{config['output_path']}/logs/convert_json_to_root_TT_ZZZH.log"

use rule convert_json_to_root_TT as convert_json_to_root_mix_ZZZH with:
    input: f"{config['output_path']}/histMixedData_ZZZH.json"
    output: f"{config['output_path']}/histMixedData_ZZZH.root"
    log: f"{config['output_path']}/logs/convert_json_to_root_mix_ZZZH.log"

use rule convert_json_to_root_TT as convert_json_to_root_data3b_ZZZH with:
    input: f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.json"
    output: f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.root"
    log: f"{config['output_path']}/logs/convert_json_to_root_data3b_ZZZH.log"


use rule convert_json_to_root_TT as convert_json_to_root_histall with:
    input: f"{config['output_path']}/histAll.json"
    output: f"{config['output_path']}/histAll.root"
    log: f"{config['output_path']}/logs/convert_json_to_root_histall.log"

#######
### Closure fits (background systematics)
########

use rule run_two_stage_closure from stat_analysis as run_two_stage_closure_HH4b with:
    input: 
        file_TT = f"{config['output_path']}/histMixedBkg_TT.root",
        file_mix = f"{config['output_path']}/histMixedData.root",
        file_sig = f"{config['output_path']}/histAll.root",
        file_data3b = f"{config['output_path']}/histMixedBkg_data_3b_for_mixed.root"
    output: f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    container: config["combine_container"]
    params:
        outputPath = f"{config['output_path']}/closureFits/",
        rebin = "1",
        variable = "SvB_MA_ps_hh",
        extra_arguments = "--use_kfold",
        container_wrapper = config['container_wrapper']
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    log: f"{config['output_path']}/logs/run_two_stage_closure_HH4b.log"

use rule run_two_stage_closure_HH4b as run_two_stage_closure_ZZ4b with:
    input: 
        file_TT = f"{config['output_path']}/histMixedBkg_ZZZH_TT.root",
        file_mix = f"{config['output_path']}/histMixedData_ZZZH.root",
        file_sig = f"{config['output_path']}/histAll.root",
        file_data3b = f"{config['output_path']}/histMixedBkg_ZZZH_data_3b_for_mixed.root"
    output: f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zz/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zz_rebin1.pkl"
    params:
        outputPath = f"{config['output_path']}/closureFits",
        rebin = "1",
        variable = "SvB_MA_ps_zz",
        extra_arguments = "",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/run_two_stage_closure_ZZ4b.log"

use rule run_two_stage_closure_ZZ4b as run_two_stage_closure_ZH4b with:
    output: f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zh_rebin1.pkl"
    params:
        outputPath = f"{config['output_path']}/closureFits",
        rebin = "1",
        variable = "SvB_MA_ps_zh",
        extra_arguments = "",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/run_two_stage_closure_ZH4b.log"

########
### Making combine inputs and datacards
########

use rule make_combine_inputs from stat_analysis as make_combine_inputs_HH4b with:
    input:
        injson = f"{config['output_path']}/histAll.json",
        injsonsyst = f"{config['output_path']}/histAll_signals__HH4b.json", 
        bkgsyst = f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    output: f"{config['output_path']}/datacards/HH4b/datacard__HH4b.txt"
    container: config["combine_container"]
    params:
        variable= "SvB_MA.ps_hh",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/HH4b.yml",
        output_dir=f"{config['output_path']}/datacards/HH4b/",
        variable_binning="",
        stat_only="",
        signal="HH4b",
        container_wrapper = config['container_wrapper']
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: f"{config['output_path']}/logs/make_combine_inputs_HH4b.log"

use rule make_combine_inputs_HH4b as make_combine_inputs_ZZ4b with:
    input:
        injson = f"{config['output_path']}/histAll.json",
        injsonsyst = f"{config['output_path']}/histAll_signals__ZZ4b.json", 
        bkgsyst = f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zz/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zz_rebin1.pkl"
    output: f"{config['output_path']}/datacards/ZZ4b/datacard__ZZ4b.txt"
    params:
        variable= "SvB_MA.ps_zz",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/ZZ4b.yml",
        output_dir=f"{config['output_path']}/datacards/ZZ4b/",
        variable_binning="",
        stat_only="",
        signal="ZZ4b",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/make_combine_inputs_ZZ4b.log"

use rule make_combine_inputs_HH4b as make_combine_inputs_ZH4b with:
    input:
        injson = f"{config['output_path']}/histAll.json",
        injsonsyst = f"{config['output_path']}/histAll_signals__ZH4b.json", 
        bkgsyst = f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/zh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_zh_rebin1.pkl"
    output: f"{config['output_path']}/datacards/ZH4b/datacard__ZH4b.txt"
    params:
        variable= "SvB_MA.ps_zh",
        rebin=1,
        metadata="python/stats_python/analysis/metadata/ZH4b.yml",
        output_dir=f"{config['output_path']}/datacards/ZH4b/",
        variable_binning="",
        stat_only="",
        signal="ZH4b",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/make_combine_inputs_ZH4b.log"

use rule workspace from combine as workspace with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{channel}}.txt"
    output: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        othersignal_maps=lambda wildcards: additional_poi(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: f"{config['output_path']}/logs/workspace_{{channel}}__{{signallabel}}.log"

use rule limits from combine as limits with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: 
        txt=f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.txt",
        json=f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.json"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: f"{config['output_path']}/logs/limits_{{channel}}__{{signallabel}}.log"

use rule significance from combine as significance with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/significance__{{signallabel}}.log"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: f"{config['output_path']}/logs/significance_{{channel}}__{{signallabel}}.log"

use rule impacts from combine as impacts with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/impacts__{{signallabel}}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        set_parameters_ranges=lambda wildcards: set_parameters_ranges(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    log: f"{config['output_path']}/logs/impacts_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule likelihood_scan from combine as likelihood_scan with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/likelihood_scan__{{signallabel}}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: f"{config['output_path']}/logs/likelihood_scan_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule gof from combine as gof with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/gof__{{signallabel}}.pdf"
    container: config["combine_container"]
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    log: f"{config['output_path']}/logs/gof_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule make_syst_plots from stat_analysis as make_syst_plots_HH4b with:
    input: f"{config['output_path']}/datacards/HH4b/datacard__ggHH_kl_1_kt_1_13p0TeV_hbbhbb.txt"
    output: f"{config['output_path']}/datacards/HH4b/systs/SvB_MA_ps_hh_nominal.pdf"
    container: config["combine_container"]
    log: f"{config['output_path']}/logs/make_syst_plots_HH4b.log"
    params:
        variable="SvB_MA_ps_hh",
        output_dir=f"{config['output_path']}/datacards/HH4b/",
        channel="HH4b",
        signal="GluGluToHHTo4B_cHHH1",
        container_wrapper = config['container_wrapper']
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"

use rule make_syst_plots_HH4b as make_syst_plots_ZZ4b with:
    input: f"{config['output_path']}/datacards/ZZ4b/datacard__ZZ_bbbb.txt"
    output: f"{config['output_path']}/datacards/ZZ4b/systs/SvB_MA_ps_zz_nominal.pdf"
    params:
        variable="SvB_MA_ps_zz",
        output_dir=f"{config['output_path']}/datacards/ZZ4b/",
        channel="ZZ4b",
        signal="ZZ4b",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/make_syst_plots_ZZ4b.log"

use rule make_syst_plots_HH4b as make_syst_plots_ZH4b with:
    input: f"{config['output_path']}/datacards/ZH4b/datacard__ZH_bbbb.txt"
    output: f"{config['output_path']}/datacards/ZH4b/systs/SvB_MA_ps_zh_nominal.pdf"
    params:
        variable="SvB_MA_ps_zh",
        output_dir=f"{config['output_path']}/datacards/ZH4b/",
        channel="ZH4b",
        signal="ZH4b",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/make_syst_plots_ZH4b.log"

use rule postfit from combine as postfit_HH4b with:
    input: f"{config['output_path']}/datacards/HH4b/datacard__ggHH_kl_1_kt_1_13p0TeV_hbbhbb.root"
    output: f"{config['output_path']}/datacards/HH4b/plots/postfitplots__ggHH_kl_1_kt_1_13p0TeV_hbbhbb__prefit.pdf"
    container: config["combine_container"]
    log: f"{config['output_path']}/logs/postfit__HH4b.log"
    params:
        signallabel="ggHH_kl_1_kt_1_13p0TeV_hbbhbb",
        channel="HH4b",
        signal="GluGluToHHTo4B_cHHH1",
        set_parameters_zero=lambda wildcards: set_parameters_zero("HH4b"),
        freeze_parameters=lambda wildcards: freeze_parameters("HH4b"),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"

use rule postfit_HH4b as postfit_ZH4b with:
    input: f"{config['output_path']}/datacards/ZH4b/datacard__ZH_bbbb.root"
    output: f"{config['output_path']}/datacards/ZH4b/plots/postfitplots__ZH_bbbb__prefit.pdf"
    log: f"{config['output_path']}/logs/postfit__ZH4b.log"
    params:
        signallabel="ZH_bbbb",
        channel="ZH4b",
        signal="ZH4b",
        set_parameters_zero=lambda wildcards: set_parameters_zero("ZH4b"),
        freeze_parameters=lambda wildcards: freeze_parameters("ZH4b"),
        container_wrapper=config.get("container_wrapper", "./run_container combine")

use rule postfit_HH4b as postfit_ZZ4b with:
    input: f"{config['output_path']}/datacards/ZZ4b/datacard__ZZ_bbbb.root"
    output: f"{config['output_path']}/datacards/ZZ4b/plots/postfitplots__ZZ_bbbb__prefit.pdf"
    log: f"{config['output_path']}/logs/postfit__ZZ4b.log"
    params:
        signallabel="ZZ_bbbb",
        channel="ZZ4b",
        signal="ZZ4b",
        set_parameters_zero=lambda wildcards: set_parameters_zero("ZZ4b"),
        freeze_parameters=lambda wildcards: freeze_parameters("ZZ4b"),
        container_wrapper=config.get("container_wrapper", "./run_container combine")