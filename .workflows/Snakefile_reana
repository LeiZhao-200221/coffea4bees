from datetime import datetime

TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')
analysis_container = "gitlab-registry.cern.ch/cms-cmu/coffea4bees:fd85472f"   ### actual tag for reproducibility
combine_container = "gitlab-registry.cern.ch/cms-analysis/general/combine-container:CMSSW_11_3_4-combine_v9.1.0-harvester_v2.1.0"
# analysis_container = "/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"   ### for when the container does not work 
# combine_container = "/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cms-analysis/general/combine-container:CMSSW_11_3_4-combine_v9.1.0-harvester_v2.1.0"
dataset_location = "metadata/datasets_HH4b.yml"

#
# rule all sets the entire workflow. This is were you define the last output of the workflow.
# Snakemake will go backawrds and check what rules does the workflow need to get the output.
#
rule all:
    input:
        "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf",
        "output/datacards/shapes.root",
        "output/tmp.log",
        'output/datacards/impacts_combine_SvB_MA_exp_HH.pdf',
        # "output/datacards/plots/SvB_postfit.png"   ### Blinded analysis, no postfits.
        # "output/datacards/kappa_scan.pdf"


rule analysis_databkgs:
    output: "output/hist_databkgs.coffea" 
    container: analysis_container
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset'],
        iy=config['year'],
        output="hist_databkgs.coffea",
        logname="hist_databkgs",
        metadata="analysis/metadata/HH4b.yml"
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
echo "Blinding SR region"
sed -i 's/blind.*/blind: true/' python/analysis/metadata/HH4b.yml
echo "Running {params.isam} {params.iy} - output {output}"
cd python/ 
mprof run -C -o mprofile_{params.logname}.dat python runner.py -d {params.isam} -p analysis/processors/processor_HH4b.py -y {params.iy} -o ../{output} -op ../output/ -m {dataset_location} --githash {params.hash} --gitdiff {params.diff} -c {params.metadata} 
cd ../
mkdir -p output/performance/
mprof plot -o output/performance/mprofile_{params.logname}.png python/mprofile_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp gitdiff.txt output/   ### only one time
#cp /tmp/coffea4bees-dask-report-* ../output/coffea4bees-dask-report_{params.logname}_{params.iy}.html
        """

use rule analysis_databkgs as analysis_signals_mix with:
    output: "output/hist_signal.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset_signals'],
        iy=config['year'],
        output="hist_signal.coffea",
        logname="hist_signal",
        metadata="analysis/metadata/HH4b.yml"

use rule analysis_databkgs as analysis_mixedbkg_data3b with:
    output: "output/histMixedBkg_data_3b_for_mixed.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="data_3b_for_mixed",
        iy=config['year_preUL'],
        output="histMixedBkg_data_3b_for_mixed.coffea",
        logname="mixedbkg_data3b",
        metadata="analysis/metadata/HH4b_mixed_data.yml"

use rule analysis_databkgs as analysis_mixedbkg with:
    output: "output/histMixedBkg_TT.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam=config['dataset_for_mixed'],
        iy=config['year'],
        output="histMixedBkg_TT.coffea",
        logname="mixedbkg_TT",
        metadata="analysis/metadata/HH4b.yml"

use rule analysis_databkgs as analysis_mixeddata with:
    output: "output/histMixedData.coffea"
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="mixeddata",
        iy=config['year_preUL'],
        output="histMixedData.coffea",
        logname="mixeddata",
        metadata="analysis/metadata/HH4b.yml"

use rule analysis_databkgs as analysis_systematics with:
    output:
        "output/histsyst_{samplesyst}-{iysyst}.coffea"
    container: analysis_container
    params:
        hash=config['hash'],
        diff=config['diff'],
        isam="{samplesyst}",
        iy="{iysyst}",
        output="histsyst_{samplesyst}-{iysyst}.coffea",
        logname="syst_{samplesyst}-{iysyst}",
        metadata="analysis/metadata/HH4b_systematics.yml"


rule merging_coffea_files_databkgs:
   input:
        files = expand(['output/histsyst_{idatsyst}-{iyear}.coffea'], idatsyst=config['dataset_systematics'], iyear=config['year']) 
   output: 
       "output/histAll_signals_cHHHX.coffea"
   container: "docker://gitlab-registry.cern.ch/cms-cmu/coffea4bees:latest"
   params:
        output= "histAll_signals_cHHHX.coffea",
        logname= "signals_cHHHX"
   resources:
       kerberos=True,
       compute_backend="kubernetes",
       kubernetes_memory_limit="9.5Gi"
   shell:
       """
mprof run -C -o python/mprofile_merge_{params.logname}.dat python python/analysis/merge_coffea_files.py -f {input.files} -o {output}
mprof plot -o output/performance/mprofile_merge_{params.logname}.png python/mprofile_merge_{params.logname}.dat
xrdcp {output} root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
cp {output} /eos/user/a/algomez/tmpFiles/XX4b/reana/{TIMESTAMP}/{params.output}
       """

use rule merging_coffea_files_databkgs as merging_coffea_files_histAll with:
    input:
        files = [ 'output/hist_signal.coffea', 'output/hist_databkgs.coffea' ]
    output: "output/histAll.coffea"
    params:
        output= "histAll.coffea",
        logname= "histAll"

rule make_plots:
    input:
        "output/histAll.coffea"
    output:
        "output/plots/RunII/passPreSel/fourTag/SB/nPVs.pdf"
    container: analysis_container
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
        cd python/ 
        mprof run -C -o mprofile_makeplots.dat python analysis/makePlots.py ../output/histAll.coffea -o ../output/plots/ -m analysis/metadata/plotsAll.yml -s xW
        mprof plot -o ../output/performance/mprofile_makeplots.png ../output/mprofile_makeplots.dat
        python .php-plots/bin/pb_deploy_plots.py ../output/plots/RunII/ /eos/user/a/algomez/work/HH4b/reana/{TIMESTAMP}/ -r -c -j 4
        """
        
rule convert_hist_to_json:
    input:
        inall = "output/histAll.coffea",
        insystcHHHX = "output/histAll_signals_cHHHX.coffea",
        inmixdata3b = "output/histMixedBkg_data_3b_for_mixed.coffea",
        inmixbkgtt = "output/histMixedBkg_TT.coffea",
        inmixdata = "output/histMixedData.coffea",
        insignal = "output/hist_signal.coffea"
    output:
        outall = "output/histAll.json",
        outsystcHHHX = "output/histAll_signals_cHHHX.json",
        outmixdata3b = "output/histMixedBkg_data_3b_for_mixed.json",
        outmixbkgtt = "output/histMixedBkg_TT.json",
        outmixdata = "output/histMixedData.json",
        outsignal = "output/hist_signal.json"
    container: analysis_container
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
python python/stats_analysis/convert_hist_to_json.py -o {output.outall} -i {input.inall}
python python/stats_analysis/convert_hist_to_json.py -o {output.outsystcHHHX} -i {input.insystcHHHX} -s
python python/stats_analysis/convert_hist_to_json.py -o {output.outmixdata3b} -i {input.inmixdata3b}
python python/stats_analysis/convert_hist_to_json.py -o {output.outmixbkgtt} -i {input.inmixbkgtt}
python python/stats_analysis/convert_hist_to_json.py -o {output.outmixdata} -i {input.inmixdata}
python python/stats_analysis/convert_hist_to_json.py -o {output.outsignal} -i {input.insignal}
        """

rule convert_json_to_root:
    input: 
      file_TT = "output/histMixedBkg_TT.json",
      file_mix = "output/histMixedData.json",
      file_sig = "output/hist_signal.json",
      file_data3b = "output/histMixedBkg_data_3b_for_mixed.json"
    container: combine_container
    output: 
        "output/histMixedBkg_TT.root",
        "output/histMixedData.root",
        "output/hist_signal.root",
        "output/histMixedBkg_data_3b_for_mixed.root"
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
      """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_TT} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_mix} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_sig} --output output/
        python3 python/stats_analysis/convert_json_to_root.py -f {input.file_data3b} --output output/
        """

rule run_two_stage_closure:
    input: 
      file_TT = "output/histMixedBkg_TT.root",
      file_mix = "output/histMixedData.root",
      file_sig = "output/hist_signal.root",
      file_data3b = "output/histMixedBkg_data_3b_for_mixed.root"
    container: combine_container
    output: "output/closureFits/ULHH_kfold/3bDvTMix4bDvT/SvB_MA/rebin2/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin2.pkl"
    params:
      outputPath = "output/closureFits/ULHH_kfold",
      rebin = "2"
    resources:
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
      """
        source /cvmfs/cms.cern.ch/cmsset_default.sh
        cd /home/cmsusr/CMSSW_11_3_4/
        cmsenv || true
        cd -
        python3 python/stats_analysis/runTwoStageClosure.py  --var SvB_MA_ps_hh  --rebin {params.rebin} \
            --outputPath {params.outputPath} \
            --input_file_TT {input.file_TT} \
            --input_file_mix {input.file_mix} \
            --input_file_sig {input.file_sig} \
            --input_file_data3b {input.file_data3b}"
        """



rule make_combine_inputs:
    input:
        injson = "output/histAll.json",
        injsonsyst = "output/histAll_signals_cHHHX.json",
        bkgsyst = "output/closureFits/ULHH_kfold/3bDvTMix4bDvT/SvB_MA/rebin2/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin2.pkl"
    output:
        "output/datacards/shapes.root",
        "output/datacards/datacard.txt"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
python3 python/stats_analysis/make_combine_inputs.py --classifier SvB_MA -f {input.injson} --syst_file {input.injsonsyst} --bkg_syst_file {input.bkgsyst} --output_dir output/datacards/stat_only/ --rebin 2 --metadata python/stats_analysis/metadata/HH4b.yml --stat_only
python3 python/stats_analysis/make_combine_inputs.py --classifier SvB_MA -f {input.injson} --syst_file {input.injsonsyst} --bkg_syst_file {input.bkgsyst} --output_dir output/datacards/ --rebin 2 --metadata python/stats_analysis/metadata/HH4b.yml --make_syst_plots 
        """

rule run_combine:
    input:
        "output/datacards/datacard.txt"
    output:
        "output/datacards/datacard.root"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
cat {input}
echo "RUNNING COMBINE"
source python/stats_analysis/run_combine.sh output/datacards/stat_only --limits
source python/stats_analysis/run_combine.sh output/datacards/ --limits
        """

rule run_impacts:
    input:
        "output/datacards/datacard.root"
    output:
        "output/datacards/impacts_combine_SvB_MA_exp_HH.pdf"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="9.5Gi"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
cat {input}
echo "RUNNING COMBINE"
source python/stats_analysis/run_combine.sh output/datacards/ --impacts
        """

rule run_postfit:
    input:
        "output/datacards/datacard.root"
    output:
        "output/datacards/plots/SvB_postfit.png"
    container: combine_container
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
source /cvmfs/cms.cern.ch/cmsset_default.sh
cd /home/cmsusr/CMSSW_11_3_4/
cmsenv || true
cd -
cat {input}
echo "RUNNING COMBINE"
cd python/stats_analysis/
source run_combine.sh ../../output/datacards/ --postfit
python3 make_postfit_plot.py -i ../../output/datacards/postfit_s.root -o {output} --do_postfit False
        """

rule move_syst_plots:
    input:
        "output/datacards/shapes.root",
    log:
        "output/tmp.log"
    container: analysis_container
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"
    shell:
        """
python python/.php-plots/bin/pb_deploy_plots.py output/datacards/ /eos/user/a/algomez/work/HH4b/reana/{TIMESTAMP}/ -r -c -j 4 -e pdf > {log}
        """

rule kappa_scan:
    input: "output/datacards/datacard.root"
    output: "output/datacards/kappa_scan.pdf"
    container: "docker://docker.io/cmssw/el7:aarch64"
    resources:
        voms_proxy=True,
        kerberos=True,
        compute_backend="kubernetes"
    shell:
        """
        cd python/stats_analysis/inference/
        bash setup.sh
        law run PlotUpperLimits --version dev --datacards output/datacards/datacard.txt --xsec fb --y-log --scan-parameters kl,20,20,5
        cp data/store/PlotUpperLimits/hh_model__model_default/datacards_716fa319cb/m125.0/poi_r/dev/limits__poi_r__scan_kl_20.0_20.0_n5__params_r_gghh1.0_r_qqhh1.0_kt1.0_CV1.0_C2V1.0__fb_log.pdf ../../../output/datacards/kappa_scan.pdf
        """
