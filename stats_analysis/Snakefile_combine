config = {
    "container_wrapper": "./run_container.sh combine ",
    "analysis_container": "dummy",
    "output_path": "output/tmp",
    "channels": {
        # "HH4b": {
        #     "signallabel": "ggHH_kl_1_kt_1_13p0TeV_hbbhbb",
        #     "othersignal": "ggHH_kl_0_kt_1_13p0TeV_hbbhbb ggHH_kl_2p45_kt_1_13p0TeV_hbbhbb ggHH_kl_5_kt_1_13p0TeV_hbbhbb"
        # },
        "ZZ4b": {
            "signallabel": "ZZ_bbbb",
            "othersignal": ""
        },
        # "ZH4b": {
        #     "signallabel": "ZH_bbbb",
        #     "othersignal": ""
        # }
    },
}

module stat_analysis:
    snakefile: "../workflows/rules/stat_analysis.smk"
    config: config

module combine:
    snakefile: "../workflows/rules/combine.smk"
    config: config
include: "../workflows/helpers/common.smk"


output_patterns = [
    "{workspace}limits_{datacard}__{signallabel}.txt",
    "{workspace}significance_expected_{datacard}__{signallabel}.txt",
    "{workspace}impacts_combine_{datacard}__{signallabel}_observed.pdf",
    "{workspace}gof_{datacard}__{signallabel}.pdf",
    "{workspace}likelihood_scan_{datacard}__{signallabel}.pdf"
]

output_combine = [
    f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.json",
    f"{config['output_path']}/datacards/{{channel}}/significance__{{signallabel}}.log",
    f"{config['output_path']}/datacards/{{channel}}/impacts__{{signallabel}}.pdf",
    f"{config['output_path']}/datacards/{{channel}}/likelihood_scan__{{signallabel}}.pdf",
    f"{config['output_path']}/datacards/{{channel}}/gof__{{signallabel}}.pdf",
]

rule all:
    input:
        [expand(pattern, zip,
                channel=list(config['channels'].keys()),
                signallabel=[config['channels'][k]['signallabel'] for k in config['channels'].keys()],
                channellabel=[k.lower().split('4b')[0] for k in config['channels'].keys()])
        for pattern in output_combine],

use rule convert_hist_to_json from stat_analysis as convert_hist_to_json_histAll with:
    input: f"{config['output_path']}/histAll.coffea"
    output: f"{config['output_path']}/histAll.json"
    log: f"{config['output_path']}/logs/convert_hist_to_json_histAll.log"


use rule make_combine_inputs from stat_analysis as make_combine_inputs_HH4b with:
    input:
        injson = f"{config['output_path']}/histAll.json",
        injsonsyst = f"{config['output_path']}/histAll_signals__HH4b.json", 
        bkgsyst = f"{config['output_path']}/closureFits/3bDvTMix4bDvT/SvB_MA/rebin1/SR/hh/hists_closure_3bDvTMix4bDvT_SvB_MA_ps_hh_rebin1.pkl"
    output: f"{config['output_path']}/datacards/HH4b/datacard__HH4b.txt"
    params:
        variable= "SvB_MA.ps_hh",
        rebin=1,
        metadata="coffea4bees/stats_coffea4bees/analysis/metadata/HH4b.yml",
        output_dir=f"{config['output_path']}/datacards/HH4b/",
        variable_binning="",
        stat_only="",
        signal="HH4b",
        container_wrapper = config['container_wrapper']
    log: f"{config['output_path']}/logs/make_combine_inputs_HH4b.log"

use rule workspace from combine as workspace with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{channel}}.txt"
    output: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    params:
        signallabel="{signallabel}",
        othersignal_maps=lambda wildcards: additional_poi(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/workspace_{{channel}}__{{signallabel}}.log"

use rule limits from combine as limits with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: 
        txt=f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.txt",
        json=f"{config['output_path']}/datacards/{{channel}}/limits__{{signallabel}}.json"
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/limits_{{channel}}__{{signallabel}}.log"

use rule significance from combine as significance with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/significance__{{signallabel}}.log"
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/significance_{{channel}}__{{signallabel}}.log"

use rule impacts from combine as impacts with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/impacts__{{signallabel}}.pdf"
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        set_parameters_ranges=lambda wildcards: set_parameters_ranges(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/impacts_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule likelihood_scan from combine as likelihood_scan with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/likelihood_scan__{{signallabel}}.pdf"
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        freeze_parameters=lambda wildcards: freeze_parameters(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/likelihood_scan_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule gof from combine as gof with:
    input: f"{config['output_path']}/datacards/{{channel}}/datacard__{{signallabel}}.root"
    output: f"{config['output_path']}/datacards/{{channel}}/gof__{{signallabel}}.pdf"
    params:
        signallabel="{signallabel}",
        set_parameters_zero=lambda wildcards: set_parameters_zero(wildcards.channel),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
    log: f"{config['output_path']}/logs/gof_{{channel}}_datacard_{{channel}}__{{signallabel}}.log"

use rule make_syst_plots from stat_analysis as make_syst_plots_HH4b with:
    input: f"{config['output_path']}/datacards/HH4b/datacard__ggHH_kl_1_kt_1_13p0TeV_hbbhbb.root"
    output: f"{config['output_path']}/datacards/HH4b/systs/SvB_MA_ps_hh_nominal.pdf"
    log: f"{config['output_path']}/logs/make_syst_plots_HH4b.log"
    params:
        variable="SvB_MA_ps_hh",
        output_dir=f"{config['output_path']}/datacards/HH4b/",
        channel="HH4b",
        signal="GluGluToHHTo4B_cHHH1",
        container_wrapper = config['container_wrapper']
    resources:
        kerberos=True,
        compute_backend="kubernetes",
        kubernetes_memory_limit="8Gi"

use rule postfit from combine as postfit_HH4b with:
    input: f"{config['output_path']}/datacards/HH4b/datacard__ggHH_kl_1_kt_1_13p0TeV_hbbhbb.root"
    output: f"{config['output_path']}/datacards/HH4b/plots/postfitplots__ggHH_kl_1_kt_1_13p0TeV_hbbhbb__prefit.pdf"
    log: f"{config['output_path']}/logs/postfit__HH4b.log"
    params:
        signallabel="ggHH_kl_1_kt_1_13p0TeV_hbbhbb",
        channel="HH4b",
        signal="GluGluToHHTo4B_cHHH1",
        log=True,
        set_parameters_zero=lambda wildcards: set_parameters_zero("HH4b"),
        freeze_parameters=lambda wildcards: freeze_parameters("HH4b"),
        container_wrapper=config.get("container_wrapper", "./run_container combine")
